{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regulated-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/midi_df.pickle', 'rb') as f:\n",
    "    songs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "united-jimmy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>seconds_length</th>\n",
       "      <th>instruments</th>\n",
       "      <th>avg_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FridayThe13th_-_MapDark.mid</td>\n",
       "      <td>93.272727</td>\n",
       "      <td>[Pipe Organ, Electric Organ, Harpsichord, Stri...</td>\n",
       "      <td>468.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv1-4.mid</td>\n",
       "      <td>41.533333</td>\n",
       "      <td>[Electric Organ, Bass, Piano, Violoncello]</td>\n",
       "      <td>149.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MM3-Snake_Man.mid</td>\n",
       "      <td>124.125874</td>\n",
       "      <td>[BASS (FINGER), Electric Bass, SYNTH BASS, Sam...</td>\n",
       "      <td>606.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dw4batl.mid</td>\n",
       "      <td>115.231788</td>\n",
       "      <td>[Electric Guitar, Bass, Electric Bass, Bass Su...</td>\n",
       "      <td>340.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Balloon_Fight_-_Main_Theme_%28Dancing_Balloon%...</td>\n",
       "      <td>11559.700000</td>\n",
       "      <td>[Steel Drum, Bass, Sampler, Synth bass, None, ...</td>\n",
       "      <td>414.571429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  seconds_length  \\\n",
       "0                        FridayThe13th_-_MapDark.mid       93.272727   \n",
       "1                                          cv1-4.mid       41.533333   \n",
       "2                                  MM3-Snake_Man.mid      124.125874   \n",
       "3                                        dw4batl.mid      115.231788   \n",
       "4  Balloon_Fight_-_Main_Theme_%28Dancing_Balloon%...    11559.700000   \n",
       "\n",
       "                                         instruments   avg_notes  \n",
       "0  [Pipe Organ, Electric Organ, Harpsichord, Stri...  468.000000  \n",
       "1         [Electric Organ, Bass, Piano, Violoncello]  149.250000  \n",
       "2  [BASS (FINGER), Electric Bass, SYNTH BASS, Sam...  606.250000  \n",
       "3  [Electric Guitar, Bass, Electric Bass, Bass Su...  340.400000  \n",
       "4  [Steel Drum, Bass, Sampler, Synth bass, None, ...  414.571429  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "headed-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = songs.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appointed-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_sampler = songs['instruments'].apply(lambda x: 'Sampler' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "liked-awareness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seconds_length</th>\n",
       "      <th>avg_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2229.000000</td>\n",
       "      <td>2229.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>101.157816</td>\n",
       "      <td>414.378286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>337.068819</td>\n",
       "      <td>502.182123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.272727</td>\n",
       "      <td>164.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>79.583333</td>\n",
       "      <td>309.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>114.352941</td>\n",
       "      <td>523.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11559.700000</td>\n",
       "      <td>11872.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       seconds_length     avg_notes\n",
       "count     2229.000000   2229.000000\n",
       "mean       101.157816    414.378286\n",
       "std        337.068819    502.182123\n",
       "min          0.000000      0.636364\n",
       "25%         47.272727    164.250000\n",
       "50%         79.583333    309.000000\n",
       "75%        114.352941    523.750000\n",
       "max      11559.700000  11872.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs[with_sampler].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "worthy-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_seconds = songs['seconds_length'].between(50, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alert-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_notes = songs['avg_notes'].between(165, 310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "valued-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_average_songs = songs[with_sampler & average_seconds & average_notes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "electronic-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = most_average_songs.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "administrative-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = list(sample['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spread-mission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UlimatepasswordmetalgearNES.mid',\n",
       " 'RM4-TitleTheme-X.mid',\n",
       " 'BubblemanByCryogen.mid',\n",
       " 'ff1temp2.mid',\n",
       " 'SMB3_-_Grass_Land.mid',\n",
       " 'StarTropics-miracola.mid',\n",
       " 'Master6.mid',\n",
       " 'Bombman1.mid',\n",
       " 'mario3-world4.mid',\n",
       " 'lemtensp.mid',\n",
       " 'Klxfairy.mid',\n",
       " 'Wiztest.mid',\n",
       " 'Woodmandrums.mid',\n",
       " 'nes_po_sm03.mid',\n",
       " 'Action52CityofDoom.mid',\n",
       " '8_Eyes.mid',\n",
       " 'da_smb3-underwater.mid',\n",
       " '4end.mid',\n",
       " 'BlasterMaster.mid',\n",
       " 'MCkids-level1.mid',\n",
       " 'tsb10.mid',\n",
       " 'z2title.mid',\n",
       " 'Golgo_13_Mafat_Conspiracy-Credits.mid',\n",
       " 'normbatl.mid',\n",
       " 'ptomato.mid']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "developed-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, instrument, note, chord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "successful-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "music21_objects = []\n",
    "\n",
    "for file in random_sample:\n",
    "    music21_objects.append(converter.parse(f'midi_files/{file}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bulgarian-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_parts = []\n",
    "\n",
    "for object in music21_objects:\n",
    "    score = instrument.partitionByInstrument(object)\n",
    "    parts = score.parts\n",
    "    for part in parts:\n",
    "        if part.getInstrument().instrumentName == 'Sampler':\n",
    "            sampler_parts.append(part.notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "geographic-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = []\n",
    "\n",
    "for sampler_part in sampler_parts:\n",
    "    for element in sampler_part.recurse():\n",
    "        if isinstance(element, note.Note):\n",
    "            notes.append(str(element.pitch) + \" \" +  str(element.quarterLength))\n",
    "        elif isinstance(element, chord.Chord):\n",
    "            notes.append('.'.join(str(n) for n in element.normalOrder) + \" \" + str(element.quarterLength))\n",
    "        elif isinstance(element, note.Rest):\n",
    "            notes.append(str(element.name)  + \" \" + str(element.quarterLength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "broken-medicine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6555"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "overhead-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 100\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "     # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / n_vocab\n",
    "\n",
    "    network_output = utils.to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "commercial-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, Bidirectional, Flatten\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def create_network(network_input, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = keras.Sequential([\n",
    "#         keras.layers.Embedding(input_dim=len(network_input), output_dim=n_vocab),\n",
    "        LSTM(256,\n",
    "             input_shape=(network_input.shape[1], network_input.shape[2]), #n_time_steps, n_features?\n",
    "             return_sequences=True\n",
    "            ),\n",
    "        LSTM(256, return_sequences=True),\n",
    "        Flatten(),\n",
    "        Dense(n_vocab, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(model, network_input, network_output):\n",
    "    \"\"\" train the neural network \"\"\"\n",
    "    \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"training_1/cp.ckpt\",\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "\n",
    "    model.fit(network_input, network_output, epochs=25, batch_size=64, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "vertical-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(notes, n_vocab):\n",
    "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
    "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "    global model\n",
    "    model = create_network(network_input, n_vocab)\n",
    "    train(model, network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "failing-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "101/101 [==============================] - 41s 387ms/step - loss: 5.6113\n",
      "\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "Epoch 2/25\n",
      "101/101 [==============================] - 39s 385ms/step - loss: 4.9656\n",
      "\n",
      "Epoch 00002: saving model to training_1/cp.ckpt\n",
      "Epoch 3/25\n",
      "101/101 [==============================] - 38s 377ms/step - loss: 4.5136\n",
      "\n",
      "Epoch 00003: saving model to training_1/cp.ckpt\n",
      "Epoch 4/25\n",
      "101/101 [==============================] - 39s 383ms/step - loss: 3.9912\n",
      "\n",
      "Epoch 00004: saving model to training_1/cp.ckpt\n",
      "Epoch 5/25\n",
      "101/101 [==============================] - 38s 380ms/step - loss: 3.4155\n",
      "\n",
      "Epoch 00005: saving model to training_1/cp.ckpt\n",
      "Epoch 6/25\n",
      "101/101 [==============================] - 38s 381ms/step - loss: 2.7995\n",
      "\n",
      "Epoch 00006: saving model to training_1/cp.ckpt\n",
      "Epoch 7/25\n",
      "101/101 [==============================] - 38s 378ms/step - loss: 2.3228\n",
      "\n",
      "Epoch 00007: saving model to training_1/cp.ckpt\n",
      "Epoch 8/25\n",
      "101/101 [==============================] - 42s 419ms/step - loss: 1.8423\n",
      "\n",
      "Epoch 00008: saving model to training_1/cp.ckpt\n",
      "Epoch 9/25\n",
      "101/101 [==============================] - 48s 481ms/step - loss: 1.4841\n",
      "\n",
      "Epoch 00009: saving model to training_1/cp.ckpt\n",
      "Epoch 10/25\n",
      "101/101 [==============================] - 48s 477ms/step - loss: 1.0712\n",
      "\n",
      "Epoch 00010: saving model to training_1/cp.ckpt\n",
      "Epoch 11/25\n",
      "101/101 [==============================] - 48s 474ms/step - loss: 0.7346\n",
      "\n",
      "Epoch 00011: saving model to training_1/cp.ckpt\n",
      "Epoch 12/25\n",
      "101/101 [==============================] - 46s 452ms/step - loss: 0.4794\n",
      "\n",
      "Epoch 00012: saving model to training_1/cp.ckpt\n",
      "Epoch 13/25\n",
      "101/101 [==============================] - 45s 443ms/step - loss: 0.3176\n",
      "\n",
      "Epoch 00013: saving model to training_1/cp.ckpt\n",
      "Epoch 14/25\n",
      "101/101 [==============================] - 42s 420ms/step - loss: 0.2377\n",
      "\n",
      "Epoch 00014: saving model to training_1/cp.ckpt\n",
      "Epoch 15/25\n",
      "101/101 [==============================] - 42s 419ms/step - loss: 0.2038\n",
      "\n",
      "Epoch 00015: saving model to training_1/cp.ckpt\n",
      "Epoch 16/25\n",
      "101/101 [==============================] - 42s 419ms/step - loss: 0.1568\n",
      "\n",
      "Epoch 00016: saving model to training_1/cp.ckpt\n",
      "Epoch 17/25\n",
      "101/101 [==============================] - 42s 418ms/step - loss: 0.1349\n",
      "\n",
      "Epoch 00017: saving model to training_1/cp.ckpt\n",
      "Epoch 18/25\n",
      "101/101 [==============================] - 42s 414ms/step - loss: 0.1457\n",
      "\n",
      "Epoch 00018: saving model to training_1/cp.ckpt\n",
      "Epoch 19/25\n",
      "101/101 [==============================] - 42s 418ms/step - loss: 0.1030\n",
      "\n",
      "Epoch 00019: saving model to training_1/cp.ckpt\n",
      "Epoch 20/25\n",
      "101/101 [==============================] - 44s 436ms/step - loss: 0.0937\n",
      "\n",
      "Epoch 00020: saving model to training_1/cp.ckpt\n",
      "Epoch 21/25\n",
      "101/101 [==============================] - 45s 441ms/step - loss: 0.0749\n",
      "\n",
      "Epoch 00021: saving model to training_1/cp.ckpt\n",
      "Epoch 22/25\n",
      "101/101 [==============================] - 45s 441ms/step - loss: 0.0960\n",
      "\n",
      "Epoch 00022: saving model to training_1/cp.ckpt\n",
      "Epoch 23/25\n",
      "101/101 [==============================] - 45s 443ms/step - loss: 0.0657\n",
      "\n",
      "Epoch 00023: saving model to training_1/cp.ckpt\n",
      "Epoch 24/25\n",
      "101/101 [==============================] - 45s 442ms/step - loss: 0.0812\n",
      "\n",
      "Epoch 00024: saving model to training_1/cp.ckpt\n",
      "Epoch 25/25\n",
      "101/101 [==============================] - 45s 443ms/step - loss: 0.0501\n",
      "\n",
      "Epoch 00025: saving model to training_1/cp.ckpt\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(set(notes))\n",
    "train_network(notes, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hawaiian-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_sequences_output(notes, pitchnames, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    # map between notes and integers and back\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    sequence_length = 100\n",
    "    network_input = []\n",
    "    output = []\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    normalized_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "\n",
    "    return (network_input, normalized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ambient-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import stream\n",
    "\n",
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output\n",
    "\n",
    "def create_midi(prediction_output):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        pattern = pattern.split()\n",
    "        temp = pattern[0]\n",
    "        duration = pattern[1]\n",
    "        pattern = temp\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a rest\n",
    "        elif('rest' in pattern):\n",
    "            new_rest = note.Rest(pattern)\n",
    "            new_rest.offset = offset\n",
    "            new_rest.storedInstrument = instrument.Piano() #???\n",
    "            output_notes.append(new_rest)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += convert_to_float(duration)\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp='test_output.mid')\n",
    " \n",
    "# From: https://stackoverflow.com/questions/1806278/convert-fraction-to-float\n",
    "def convert_to_float(frac_str):\n",
    "    try:\n",
    "        return float(frac_str)\n",
    "    except ValueError:\n",
    "        num, denom = frac_str.split('/')\n",
    "        try:\n",
    "            leading, num = num.split(' ')\n",
    "            whole = float(leading)\n",
    "        except ValueError:\n",
    "            whole = 0\n",
    "        frac = float(num) / float(denom)\n",
    "        return whole - frac if whole < 0 else whole + frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "suburban-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchnames = sorted(set(item for item in notes))\n",
    "# Get all pitch names\n",
    "n_vocab = len(set(notes))\n",
    "\n",
    "network_input, normalized_input = prepare_sequences_output(notes, pitchnames, n_vocab)\n",
    "prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "create_midi(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-consciousness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
